{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "30279bd3-481b-4ca7-a20b-350d8334ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374840ff",
   "metadata": {},
   "source": [
    "# Predicting the temperament of ROUSes using k-NN classification\n",
    "Using other data we have in the table, we want to predict the temperament of ROUSes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = pd.read_csv('../data/ROUSes.csv')\n",
    "print(rouses.shape)\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b1381",
   "metadata": {},
   "source": [
    "### Exploratory analysis\n",
    "First, let's look at a scatterplot with the temperament represented as color and symbols to get a general idea of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=rouses, x='Age',y='Length', hue='Temperament', style='Temperament')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe1a78",
   "metadata": {},
   "source": [
    "As you can see, there are some clusters of the same temperament, which means the samples have the same temperament as their neighbors, so k-NN should work well for those.  But there are also definitely some samples are more \"alone\" so k-NN won't be as good for prediction.\n",
    "\n",
    "The next cell normalizes the columns so the neighbor distance calculations will be scaled equivalently.  You can try skipping this cell to see the performance without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, try skipping this cell and see results without scaling\n",
    "rouses['Age'] = (rouses['Age']-rouses['Age'].min())/( rouses['Age'].max()-rouses['Age'].min()) # normalize 'Age' columns\n",
    "rouses['Length'] = (rouses['Length']-rouses['Length'].min())/( rouses['Length'].max()-rouses['Length'].min()) # normalize 'Length' columns\n",
    "rouses['Weight'] = (rouses['Weight']-rouses['Weight'].min())/( rouses['Weight'].max()-rouses['Weight'].min()) # normalize 'Weight' columns\n",
    "\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c5d9c3",
   "metadata": {},
   "source": [
    "Okay, as usual let's follow the train and test process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e78bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rouses.sample(frac= 0.8, random_state=1234) # 80% rows for training\n",
    "test = rouses.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbdeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Temperament']\n",
    "X_train = train.drop(columns=['Temperament'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Temperament']\n",
    "X_test = test.drop(columns=['Temperament']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678959f",
   "metadata": {},
   "source": [
    "Notice that is a very small number of train and test samples, so our results are going to be highly dependent on how the data is split.  Let's try k-NN classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c61857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score:',knn.score(X_train, y_train))\n",
    "print('Test score:',knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adbd7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)\n",
    "print('Prediction:',knn.predict(X_test))\n",
    "print('Actual:',list(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a3b12",
   "metadata": {},
   "source": [
    "Well, predicting 4 out of 6 isn't great, but it is actually better than expected considering the Train score.  Predicting temperament is a pretty tough challenge!  Try different values for k (`n_neighbors`) to see what changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db5eaf",
   "metadata": {},
   "source": [
    "# Predicting the weight of ROUSes using k-NN\n",
    "Using other data we have in the table, we want to predict the weight of ROUSes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3efcc9-a243-4d5a-b3cd-45eb4b10e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = pd.read_csv('../data/ROUSes.csv')\n",
    "print(rouses.shape)\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b92724",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=rouses, x='Length',y='Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c351d",
   "metadata": {},
   "source": [
    "In our previous linear regression work, we were able to use `Age` to predict `Weight` quite well because the correlation was close to linear.  Let's try using `Length` instead, and see how well we can predict despite the relationship being less linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd30e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = rouses.drop(columns=['Temperament','Age']) # drop the columns 'Temperament' and 'Age'\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971f586",
   "metadata": {},
   "source": [
    "Train and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93210d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rouses.sample(frac= 0.8, random_state=4321) # 80% rows for training\n",
    "test = rouses.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f889f-74a6-4c07-a931-a0d713a3d3a7",
   "metadata": {},
   "source": [
    "The next thing to do is to separate out the target data `Weight` from the predictor data (everything else; in this case just `Length` is left)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2e86e-e37d-4bcd-90b1-92493b91762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Weight']\n",
    "X_train = train.drop(columns=['Weight'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Weight']\n",
    "X_test = test.drop(columns=['Weight']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e7adb5-97c4-401f-9615-4f97918ba99d",
   "metadata": {},
   "source": [
    "Okay, let's try using weighted K-NN for regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a59769-1d00-4e90-90b5-8360edbff630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5,weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score:',knn.score(X_train, y_train))\n",
    "print('Test score:',knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7f7a2",
   "metadata": {},
   "source": [
    "For regression a \"score\" (the R2 value) near 1 is what we are hoping for, and 0 is the worst result.  So our model is doing a very good job at predicting the data!\n",
    "\n",
    "To visualize, we can plug in the test values in and have their outputs predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(train,x='Length', y='Weight', label = \"training data\")\n",
    "ax.scatter(test['Length'], predictions, label = \"predictions\")\n",
    "ax.set(xlabel='Length', ylabel='Weight')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b5532",
   "metadata": {},
   "source": [
    "Indeed, it looks like these predictions follow the trend of the data! You can try different values for k to see how the results change. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63544b3b-a930-419b-a9c2-a6d41a7b279e",
   "metadata": {},
   "source": [
    "# One-hot encoding\n",
    "\n",
    "Let's try adding in `Temperament` along with `Length` to help predict `Weight`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7414a-eeb7-485a-ae3f-8a9940e89e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses = pd.read_csv('../data/ROUSes.csv')\n",
    "rouses=rouses.drop(columns=['Age'])\n",
    "rouses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7d28c",
   "metadata": {},
   "source": [
    "Let's see how weight varies for different `Temperament`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=rouses, x='Weight',y='Temperament')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb40e68-d26f-45fd-a2c7-33b3a8e25c52",
   "metadata": {},
   "source": [
    "Uh-oh, `Temperament` might not much help to predict `Weight`!  That range of `Weight` for the `Sleepy` category means that `Temperament` won't be a very good predictor for `Weight` for ROUSes in that category, and it might actually end up hurting our predictions.  But the tighter range for `No-nonsense` is more promising.  \n",
    "\n",
    "As we learned in lecture, to do distance calculations for a categorical feature we'll use one-hot encoding to convert `Temperament` to 4 new columns.  Also, we need to normalize the `Length` feature to make similar scales.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1377376-e4dc-4fbe-a8f0-7a9bab93dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouses=pd.get_dummies(rouses,dtype=int)\n",
    "rouses['Length']= (rouses['Length']-rouses['Length'].min())/(rouses['Length'].max()-rouses['Length'].min()) # normalize Length\n",
    "rouses.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15598c21",
   "metadata": {},
   "source": [
    "So easy!  Okay, let's do the prediction for `Weight` again, now using `Length` and `Temperament` one-hot features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ef4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rouses.sample(frac= 0.8, random_state=4321) # 80% rows for training\n",
    "test = rouses.drop(index=train.index) # rest of rows for testing\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['Weight']\n",
    "X_train = train.drop(columns=['Weight'])\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "y_test = test['Weight']\n",
    "X_test = test.drop(columns=['Weight']) \n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e0183-34ad-439c-a4de-5de45dc588d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=5,weights='distance')\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score:',knn.score(X_train, y_train))\n",
    "print('Test score:',knn.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7268c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn.predict(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(train,x='Length', y='Weight', label = \"training data\")\n",
    "ax.scatter(test['Length'], predictions, label = \"predictions\")\n",
    "ax.set(xlabel='Length', ylabel='Weight')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0a304",
   "metadata": {},
   "source": [
    "Indeed, it looks like adding the `Temperament` feature actually hurt prediction performance.  Unlike with Linear Regression, there are no coefficients to change the importance of each feature, so it is very important to choose useful features for k-NN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
